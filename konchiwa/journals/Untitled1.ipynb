{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 11\n",
      "2 12\n",
      "3 13\n",
      "4 14\n",
      "5 15\n",
      "6 16\n",
      "7 17\n",
      "8 None\n"
     ]
    }
   ],
   "source": [
    "from itertools import zip_longest\n",
    "\n",
    "b = [1,2,3,4,5,6,7,8]\n",
    "c = ['a','','c','d','e']\n",
    "d = [11,12,13,14,15,16,17]\n",
    "for b,c,d in zip_longest(b,c,d):\n",
    "    print(b,d)\n",
    "#    z = b,c,d\n",
    " #   print(z.replace('None',''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.tandfonline.com/na101/home/literatum/publisher/tandf/journals/content/rrel20/2019/rrel20.v049.i02/rrel20.v049.i02/20190402-01/rrel20.v049.i02.cover.jpg\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import lxml\n",
    "import re\n",
    "\n",
    "ID = 'Reli'\n",
    "pub = 'Routledge'\n",
    "j_url = \"https://www.tandfonline.com/toc/rrel20/current\"\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:47.0) Gecko/20100101 Firefox/47.0\",\n",
    "    }\n",
    "HP = requests.get(j_url, headers=headers)\n",
    "soup = BeautifulSoup(HP.text, \"html.parser\")\n",
    "\n",
    "#ジャーナルタイトル\n",
    "title = soup.find(class_=\"title\").string\n",
    "#print(title)\n",
    "\n",
    "#論文タイトル\n",
    "articles = []\n",
    "p_titles = soup.find_all(class_=\"header\")\n",
    "for p_title in p_titles:\n",
    "    aaa = p_title.getText()\n",
    "    if 'Article' in aaa:\n",
    "        Reli_titles = aaa.replace(\"Article\",'').replace(\"\\n\",\"\").strip(\" \")\n",
    "        articles.append(Reli_titles)\n",
    "#print(articles)\n",
    "\n",
    "urls = []\n",
    "for p_title in p_titles:\n",
    "    aaa = p_title.getText()\n",
    "    if 'Article' in aaa:\n",
    "        b = p_title.find('a').get('href')\n",
    "        c = 'https://www.tandfonline.com' + b\n",
    "        urls.append(c)\n",
    "#print(urls)\n",
    "\n",
    "image = soup.find(class_=\"publicationCoverImage\")\n",
    "imag = image.find('img').get('src')\n",
    "img = 'https://www.tandfonline.com' + imag\n",
    "print(img)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://journals.sagepub.com/na101/home/literatum/publisher/sage/journals/content/crra/2019/crra_7_1/crra_7_1/20190318/crra_7_1.cover.png\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import lxml\n",
    "import re\n",
    "\n",
    "ID = 'CRR'\n",
    "pub = 'SAGE'\n",
    "j_url = \"https://journals.sagepub.com/toc/crra/7/1\" \n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:47.0) Gecko/20100101 Firefox/47.0\",\n",
    "    }\n",
    "HP = requests.get(j_url, headers=headers)\n",
    "HP_text = HP.text.split('Book Reviews')\n",
    "soup = BeautifulSoup(HP.text, \"html.parser\")\n",
    "soup2 = BeautifulSoup(HP_text[0+1], \"html.parser\")\n",
    "\n",
    "#雑誌タイトル\n",
    "title1 = soup.find(\"title\").string\n",
    "title = title1[:title1.find(\"-\")]\n",
    "#出版社\n",
    "pub = soup.find(class_=\"copyrightSAGELink\").getText()\n",
    "#論文タイトル\n",
    "aa = soup2.find_all(class_=\"heading-title\")\n",
    "articles = []\n",
    "for a in aa:\n",
    "    articles.append(a.getText())\n",
    "        \n",
    "#url\n",
    "urls = []\n",
    "bb = soup2.find_all('div', class_=\"art_title linkable\")\n",
    "for b in bb:\n",
    "    cc = b.find_all('a')\n",
    "    for c in cc:\n",
    "        d = c.get('href')\n",
    "        e = 'https://journals.sagepub.com' + d\n",
    "        urls.append(e)\n",
    "\n",
    "image = soup.find(class_=\"cover\")\n",
    "imag = image.find('img')\n",
    "img = 'https://journals.sagepub.com' + imag.get('src')\n",
    "print(img)\n",
    "\n",
    "\n",
    "#print(title,articles,urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import lxml\n",
    "import re\n",
    "\n",
    "def nous(jjj):\n",
    "    ID = 'Nous'\n",
    "    pub = 'Wiley'\n",
    "    j_url = jjj \n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:47.0) Gecko/20100101 Firefox/47.0\",\n",
    "        }\n",
    "    HP = requests.get(j_url, headers=headers)\n",
    "    soup = BeautifulSoup(HP.text, \"html.parser\")\n",
    "\n",
    "\n",
    "    #雑誌タイトル\n",
    "    title1 = soup.find(\"title\").string\n",
    "    title = title1[:title1.find(\"-\")]\n",
    "\n",
    "\n",
    "    print(title)\n",
    "    #論文タイトル\n",
    "    aa = soup.find_all(class_=\"issue-item__title__en\")\n",
    "    articles = []\n",
    "    for a in aa:\n",
    "        articles.append(a.getText())\n",
    "    print(articles)\n",
    "    #url\n",
    "    urls = []\n",
    "    bb = soup.find_all(class_=\"issue-item\")\n",
    "    for b in bb:\n",
    "        cc = b.find_all('a')\n",
    "        for c in cc:\n",
    "            d = c.get('href')\n",
    "            if 'epdf' in d:\n",
    "                e = 'https://onlinelibrary.wiley.com' + d\n",
    "                urls.append(e)\n",
    "    print(urls)\n",
    "\n",
    "    image = soup.find(class_=\"cover-image__image hasDetails\")\n",
    "    img = image.find('img')\n",
    "    img = imag.get('src')\n",
    "    print(img)\n",
    "\n",
    "#return (ID, title, j_url, img, pub, articles, urls)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analytic Philosophy \n",
      "['Presentism and Modal Realism', 'An Indexical Theory of Racial Pejoratives', 'Method in the Service of Progress', 'Fatalism and Future Contingents', 'Exiting The Consequentialist Circle: Two Senses of Bringing It About', 'Real Definition', 'Slurring Perspectives', 'Slurs and Stereotypes', \"It's Not What You Said, It's the Way You Said It: Slurs and Conventional Implicatures\", 'Moral and Semantic Innocence']\n",
      "['https://onlinelibrary.wiley.com/doi/epdf/10.1111/phib.12154', 'https://onlinelibrary.wiley.com/doi/epdf/10.1111/phib.12156', 'https://onlinelibrary.wiley.com/doi/epdf/10.1111/phib.12148', 'https://onlinelibrary.wiley.com/doi/epdf/10.1111/phib.12157', 'https://onlinelibrary.wiley.com/doi/epdf/10.1111/phib.12145', 'https://onlinelibrary.wiley.com/doi/epdf/10.1111/phib.12067', 'https://onlinelibrary.wiley.com/doi/epdf/10.1111/phib.12022', 'https://onlinelibrary.wiley.com/doi/epdf/10.1111/phib.12021', 'https://onlinelibrary.wiley.com/doi/epdf/10.1111/phib.12024', 'https://onlinelibrary.wiley.com/doi/epdf/10.1111/phib.12020']\n",
      "https://wol-prod-cdn.literatumonline.com/cms/attachment/d9acf614-a328-4063-9971-06ecbdcf4c16/phib.2019.60.issue-1.cover.gif\n"
     ]
    }
   ],
   "source": [
    "j_url = \"https://onlinelibrary.wiley.com/journal/2153960x\"\n",
    "nous(j_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
